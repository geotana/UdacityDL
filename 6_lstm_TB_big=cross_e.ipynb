{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "6_lstm-TB-big=cross-e.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [
        "Y5tapX3kpcqZ"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "[View in Colaboratory](https://colab.research.google.com/github/geotana/UdacityDL/blob/master/6_lstm_TB_big=cross_e.ipynb)"
      ]
    },
    {
      "metadata": {
        "id": "8tQJd2YSCfWR",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        ""
      ]
    },
    {
      "metadata": {
        "id": "D7tqLMoKF6uq",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Deep Learning\n",
        "=============\n",
        "\n",
        "Assignment 6\n",
        "------------\n",
        "\n",
        "After training a skip-gram model in `5_word2vec.ipynb`, the goal of this notebook is to train a LSTM character model over [Text8](http://mattmahoney.net/dc/textdata) data."
      ]
    },
    {
      "metadata": {
        "id": "4eErTCTybtph",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "---\n",
        "Problem 2\n",
        "---------\n",
        "\n",
        "We want to train a LSTM over bigrams, that is pairs of consecutive characters like 'ab' instead of single characters like 'a'. Since the number of possible bigrams is large, feeding them directly to the LSTM using 1-hot encodings will lead to a very sparse representation that is very wasteful computationally.\n",
        "\n",
        "a- Introduce an embedding lookup on the inputs, and feed the embeddings to the LSTM cell instead of the inputs themselves.\n",
        "\n",
        "b- Write a bigram-based LSTM, modeled on the character LSTM above.\n",
        "\n",
        "c- Introduce Dropout. For best practices on how to use Dropout in LSTMs, refer to this [article](http://arxiv.org/abs/1409.2329).\n",
        "\n",
        "---"
      ]
    },
    {
      "metadata": {
        "id": "1VijdTxF6NPq",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "18.5.15 検証結果が明らかにおかしい。\n",
        "原因予想\n",
        "h  r r a r r r a r r r a r r r a r r r a r r r \n",
        "1 学習データが非常に偏っている   => No\n",
        "\n",
        "2 推論部分のロジックがおかしい\n",
        "\n",
        "3 ab２文字にした、同じデータセットが2回回った => Yes Fixed\n",
        "\n",
        "18.5.16 状況関わらず　\n",
        "\n",
        "原因予想\n",
        "・lossがマイナスなので、根本がおかしい。Graph周り。==> 推論のインプットとアウトプを突き合わせみる 問題なし！？\n",
        "\n",
        "Average loss at step 1500: -775.086458 learning rate: 10.000000\n",
        "Minibatch perplexity: 0.00\n",
        "Validation set perplexity: 0.09\n",
        "inp [' or what by joe peac', ' some places such as'] ans ['r what by joe peacot', 'ome places such as c'] pre ['rererererererererere', 'rererererererererere']\n",
        "inp ['ott and writings by ', ' clearwater feature '] ans ['t and writings by fr', 'learwater feature re'] pre ['clclclclclclclclclcl', 'clclclclclclclclclcl']\n",
        "\n",
        "全然学習しない。\n",
        "\n",
        "18.6.21\n",
        "学習しない原因は、ネットにあるかも\n",
        "オリジナル：27文字を 27次元の1-Hot Vecで表現\n",
        "中間層は64\n",
        "10文字の並びで学習\n",
        "\n",
        "bigram版：2文字合わせて786次元を128に圧縮\n",
        "中間層は64 ==>中間層　128*2で試す\n",
        "\n",
        "実質20文字の並びで学習\n",
        "\n",
        " ==>中間層　128*2で試す\n",
        " 依然損失関数がマイナスで、増加していく。おかしい\n",
        " \n",
        " ==> unr 5で試す　１０文字の並びでの学習に相当\n",
        " 変わる兆候なし\n",
        "\n",
        "18.6.24\n",
        "forumを読んで気がついたが、このネットワークは分類問題である。\n",
        "しかし、出力が1ーHotVecになっていないので、損失関数としてクロスエントロピーを使うのが不適切である。\n",
        "解決方法\n",
        "1）出力が、N次元の点だとして、回帰問題のように、2点間の、2乗距離の和をとる\n",
        "➡️学習が進まない。\n",
        "unr = 10 * 2\n",
        "num_nodes = 64*4\n",
        "としても大差なし\n",
        "\n",
        "2）出力を1-HotVecにして比較する\n"
      ]
    },
    {
      "metadata": {
        "id": "6nscy0BCo6l6",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 202
        },
        "outputId": "d7d225fc-3cd1-4728-8ea4-43674c5fe787"
      },
      "cell_type": "code",
      "source": [
        "!ls drive/ColabData/\n",
        "!dpkg -l  software-properties-common python-software-properties module-init-tools google-drive-ocamlfuse fuse"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "ls: cannot access 'drive/ColabData/': No such file or directory\n",
            "Desired=Unknown/Install/Remove/Purge/Hold\n",
            "| Status=Not/Inst/Conf-files/Unpacked/halF-conf/Half-inst/trig-aWait/Trig-pend\n",
            "|/ Err?=(none)/Reinst-required (Status,Err: uppercase=bad)\n",
            "||/ Name           Version      Architecture Description\n",
            "+++-==============-============-============-=================================\n",
            "un  software-prope <none>       <none>       (no description available)\n",
            "\u001b[1mdpkg-query:\u001b[0m no packages found matching python-software-properties\n",
            "\u001b[1mdpkg-query:\u001b[0m no packages found matching module-init-tools\n",
            "\u001b[1mdpkg-query:\u001b[0m no packages found matching google-drive-ocamlfuse\n",
            "\u001b[1mdpkg-query:\u001b[0m no packages found matching fuse\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "6YGw1TtCyp7Z",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 104
        },
        "outputId": "c9128945-3ea6-4c7a-b3a0-d3b27617b9e7"
      },
      "cell_type": "code",
      "source": [
        "!apt-get install -y -qq software-properties-common python-software-properties module-init-tools  2>&1 > /dev/null\n",
        "!add-apt-repository -y ppa:alessandro-strada/ppa 2>&1 > /dev/null\n",
        "!apt-get update -qq 2>&1 > /dev/null\n",
        "!apt-get -y install -qq google-drive-ocamlfuse fuse  2>&1 > /dev/null\n",
        "# Generate auth tokens for Colab\n",
        "from google.colab import auth\n",
        "auth.authenticate_user()\n",
        "# Generate creds for the Drive FUSE library.\n",
        "from oauth2client.client import GoogleCredentials\n",
        "creds = GoogleCredentials.get_application_default()\n",
        "\n",
        "import getpass\n",
        "!google-drive-ocamlfuse -headless -id={creds.client_id} -secret={creds.client_secret} < /dev/null 2>&1 | grep URL\n",
        "vcode = getpass.getpass()\n",
        "!echo {vcode} | google-drive-ocamlfuse -headless -id={creds.client_id} -secret={creds.client_secret}\n",
        "# Create a directory and mount Google Drive using that directory.\n",
        "!mkdir -p drive\n",
        "!google-drive-ocamlfuse drive"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Please, open the following URL in a web browser: https://accounts.google.com/o/oauth2/auth?client_id=32555940559.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive&response_type=code&access_type=offline&approval_prompt=force\r\n",
            "··········\n",
            "Please, open the following URL in a web browser: https://accounts.google.com/o/oauth2/auth?client_id=32555940559.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive&response_type=code&access_type=offline&approval_prompt=force\n",
            "Please enter the verification code: Access token retrieved correctly.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "MvEblsgEXxrd",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "b13b1b32-1e4b-46b7-a662-577f8ca9b43d"
      },
      "cell_type": "code",
      "source": [
        "from __future__ import print_function #@title デフォルトのタイトル テキスト\n",
        "import os\n",
        "import numpy as np\n",
        "import random\n",
        "import string\n",
        "import zipfile\n",
        "import pdb #pdb.set_trace()\n",
        "from six.moves import range\n",
        "from six.moves.urllib.request import urlretrieve\n",
        "import tensorflow as tf\n",
        "\n",
        "device_name = tf.test.gpu_device_name()\n",
        "if device_name != '/device:GPU:0':\n",
        "  raise SystemError('GPU device not found')\n",
        "print('Found GPU at: {}'.format(device_name))"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found GPU at: /device:GPU:0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "RJ-o3UBUFtCw",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 185
        },
        "outputId": "17b263e5-a932-4686-ce0f-3828c878f001"
      },
      "cell_type": "code",
      "source": [
        "def maybe_download(filename, expected_bytes):\n",
        "  \"\"\"Download a file if not present, and make sure it's the right size.\"\"\"\n",
        "  if not os.path.exists(\"./drive/ColabData/\"+filename):\n",
        "    filename, _ = urlretrieve('http://mattmahoney.net/dc/' + filename, \"./drive/ColabData/\" + filename)\n",
        "  else:\n",
        "    filename = \"./drive/ColabData/\" + filename\n",
        "  statinfo = os.stat(filename)\n",
        "  if statinfo.st_size == expected_bytes:\n",
        "    print('Found and verified %s' % filename)\n",
        "  else:\n",
        "    print(statinfo.st_size)\n",
        "    raise Exception('Failed to verify ' + filename + '. Can you get to it with a browser?')\n",
        "  return filename\n",
        "\n",
        "def read_data(filename):\n",
        "  with zipfile.ZipFile(filename) as f:\n",
        "    name = f.namelist()[0]\n",
        "    data = tf.compat.as_str(f.read(name))\n",
        "  return data\n",
        "\n",
        "filename = maybe_download('text8.zip', 31344016)\n",
        "text = read_data(filename)\n",
        "print('Data size %d' % len(text))\n",
        "\n",
        "def cutprint(ltext):\n",
        "  print(len(ltext))\n",
        "  print(ltext[0:10])\n",
        "  print(ltext[10:20])\n",
        "  print(ltext[20:50])\n",
        "\n",
        "valid_size = 1000\n",
        "valid_text = text[:valid_size]\n",
        "cutprint(valid_text)\n",
        "\n",
        "train_text = text[valid_size:]\n",
        "cutprint(train_text)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found and verified ./drive/ColabData/text8.zip\n",
            "Data size 100000000\n",
            "1000\n",
            " anarchism\n",
            " originate\n",
            "d as a term of abuse first use\n",
            "99999000\n",
            "ons anarch\n",
            "ists advoc\n",
            "ate social relations based upo\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "ga2CYACE-ghb",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Create a small validation set."
      ]
    },
    {
      "metadata": {
        "id": "Zdw6i4F8glpp",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Utility functions to map characters to vocabulary IDs and back."
      ]
    },
    {
      "metadata": {
        "id": "gAL1EECXeZsD",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        },
        "outputId": "c494250e-8cb8-4002-9a0e-459ac74c6ae0"
      },
      "cell_type": "code",
      "source": [
        "final_embeddings_np_voc_em = np.load('./drive/ColabData/final_embeddings.npy')\n",
        "# voc -> vocabulary size\n",
        "# em -> embeddings\n",
        "print(\"final_embeddings.shape\",final_embeddings_np_voc_em.shape)\n",
        "\n",
        "embedding_size = 128 # Dimension of the embedding vector.\n",
        "vocabulary_size0 = 728 # num of ab bc ...\n",
        "\n",
        "import pickle\n",
        "with open('./drive/ColabData/dictionary.pickle', mode='rb') as f:\n",
        "  dictionary = pickle.load(f)\n",
        "\n",
        "def char2ab_vec_np_em_1(char1,char2):  #2文字からnp_em_1 [ [0.1] [0.15]...  ]\n",
        "  ab_seqnum = dictionary[ char1 + char2]\n",
        "  ab_np_voc_1 = np.zeros((vocabulary_size0,1))  \n",
        "  ab_np_voc_1[ab_seqnum][0] = 1.0   # 1-hot vector\n",
        "  return (final_embeddings_np_voc_em.T @ ab_np_voc_1)\n",
        "\n",
        "with open('./drive/ColabData/reverse_dictionary.pickle', mode='rb') as f:\n",
        "  reverse_dictionary = pickle.load(f)\n",
        "    \n",
        "def em_vec2ab(np_em_1): #em vectorから文字列を返す\n",
        "  argmax_1 = np.argmax( final_embeddings_np_voc_em @ np_em_1, axis = 0)\n",
        "#   pdb.set_trace()\n",
        "  return reverse_dictionary[argmax_1[0]]\n",
        "  \n",
        "print( em_vec2ab(char2ab_vec_np_em_1('q','u')), \"|\", em_vec2ab( char2ab_vec_np_em_1('i','s')) )\n"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "final_embeddings.shape (728, 128)\n",
            "qu | is\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "lFwoyygOmWsL",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Function to generate a training batch for the LSTM model."
      ]
    },
    {
      "metadata": {
        "id": "d9wMtjy5hCj9",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 104
        },
        "outputId": "eddbe7a9-b249-4223-9876-9a75be225f1d"
      },
      "cell_type": "code",
      "source": [
        "batch_size= 64 #64\n",
        "num_unrollings= 10 #10\n",
        "\n",
        "class BatchGenerator(object):\n",
        "  def __init__(self, text, batch_size, num_unrollings):\n",
        "    self._text = text\n",
        "    self._text_size = len(text)\n",
        "    self._batch_size = batch_size\n",
        "    self._num_unrollings = num_unrollings\n",
        "    segment = self._text_size // batch_size   # 1batch当たりのテキストサイズ \n",
        "    self._cursor_L64 = [ offset * segment for offset in range(batch_size)] #segmentのスタート位置\n",
        "    self._last_batch = self._next_batch() #np (bs,em)\n",
        "  \n",
        "  def _next_batch(self): #ここは良い。順番にab cd 。。。と返している\n",
        "    \"\"\"Generate a single batch from the current cursor position in the data.\"\"\"\n",
        "    batch_np_bs_em = np.empty((0, embedding_size),dtype=np.float)\n",
        "\n",
        "    for b in range(self._batch_size):\n",
        "       a_pos = self._cursor_L64[b] % self._text_size #折り返し\n",
        "       b_pos =  (a_pos + 1) % self._text_size #折り返し \n",
        "       batch_np_bs_em = np.append(batch_np_bs_em,\n",
        "           char2ab_vec_np_em_1( self._text[a_pos], self._text[b_pos]) .T, axis=0)\n",
        "#        pdb.set_trace()\n",
        "       self._cursor_L64[b] =(b_pos  + 1) %  self._text_size #segment内の次のテキスト位置をセット abを考慮、折り返し あり\n",
        "\n",
        "    return batch_np_bs_em # ab Word Vector\n",
        "  \n",
        "  def next(self): #あれ。ここも良さそう。\n",
        "    \"\"\"Generate the next array of batches from the data. The array consists of\n",
        "    the last batch of the previous array, followed by num_unrollings new ones.\n",
        "    \"\"\"\n",
        "    batches = [self._last_batch] #最初のab vecをnp_bs_emの先頭に LIST of np(bs,em)) \n",
        "    # batches[0] = ['on', 'of']  、batches[1] = ['s ', ' t'] 繋がっている\n",
        "  \n",
        "    for step in range(self._num_unrollings):\n",
        "      batches.append(self._next_batch()) #後ろにunrolling分だけ追加\n",
        "#     pdb.set_trace()  \n",
        "    self._last_batch = batches[-1] #LIST末尾の要素をセット\n",
        "    return batches #cursorから始まるunrolling+1個分のnp(bs,em)のLISTを返す\n",
        "  \n",
        "def np_bs_em2ab_li_bs(np_bs_em): # bs個のem vectorからbs個の文字列を返す\n",
        "\n",
        "  ab_li_bs=[]\n",
        "  for b in range(np_bs_em.shape[0]): \n",
        "    x_np_em = np_bs_em.T [ : , b]\n",
        "    ab_li_bs.append(em_vec2ab( x_np_em[ : , np.newaxis]))\n",
        "  return ab_li_bs\n",
        "  \n",
        "def characters(probabilities): #1-hot vecから1文字にして、bs個のリストを返す\n",
        "#   Turn a 1-hot encoding or a probability distribution over the possible\n",
        "#   characters back into its (most likely) character representation.　\n",
        "  x = [id2char(c) for c in np.argmax(probabilities, 1)]\n",
        "  return x\n",
        "  \n",
        "# def batches2string(batches): # np_bs_emのリストから文字列を返す　オリジナルなら [unr個] bs個のリスト\n",
        "#   \"\"\"Convert a sequence of batches back into their (most likely) string\n",
        "#   representation.\"\"\"\n",
        "# #  fi-embed.T x ab_vec -> voc vect 最大の要素から reverse dict\n",
        "#   string = []  #bs個の要素を持つ縦ベクトルのnum_unrollings個の要素を持つList\n",
        " \n",
        "#   for step in range(len(batches)):\n",
        "#     string.append( np_bs_em2ab_li_bs( batches[step] ))\n",
        "#   return string\n",
        "\n",
        "def batches2string(batches):\n",
        "  \"\"\"Convert a sequence of batches back into their (most likely) string\n",
        "  representation.\"\"\"\n",
        "  s = [''] * batches[0].shape[0] # bs 個の空の要素を持ったリストを作る\n",
        "  for b in batches: #unr個の繰り返し\n",
        "    s = [''.join(x) for x in zip(s, np_bs_em2ab_li_bs(b))]\n",
        "#     s = [''.join(x) for x in zip(s, characters(b))]\n",
        "  return s\n",
        "\n",
        "#next()の呼び出しで次々に教師データのバッチサイズ分の文字emベクトルを返すようなオブジェクトを作る\n",
        "train_batches = BatchGenerator(train_text, batch_size, num_unrollings) \n",
        "\n",
        "#正しく、nextで訓練データが取得できるか確かめる #精査すべし！！\n",
        "print(\"1:\", batches2string( train_batches.next() ) ) # train_batches.next()はunrolling+1個分のnp(bs,em)のLIST\n",
        "print(\"2:\", batches2string( train_batches.next() ) ) # これをunrolling+1個 x bs個の文字で返す\n",
        "# 1: [['on', 'n ', 'gn', ' d', 'of', 'at', 'st', 'ck'], ['s ', 'fr', 'if', 'ru', ' t', ' l', ' d', 'y '], ['an', 'om', 'ic', 'gs', 'he', 'ea', 'ai', 'ri'], ['ar', ' t', 'an', ' c', ' o', 'st', 'ly', 'ca'], ['ch', 'he', 't ', 'on', 'ri', ' n', ' c', 'rd'], ['is', ' n', 'th', 'fu', 'gi', 'ot', 'ol', 'o '], ['ts', 'at', 'an', 'si', 'na', ' p', 'le', 'th'], [' a', 'io', ' i', 'on', 'l ', 'ar', 'ge', 'is'], ['dv', 'na', 'n ', ' i', 'do', 'li', ' n', ' c'], ['oc', 'l ', 'je', 'na', 'cu', 'am', 'ew', 'la'], ['at', 'me', 'rs', 'bi', 'me', 'en', 'sp', 'ss']]\n",
        "# 2: [['at', 'me', 'rs', 'bi', 'me', 'en', 'sp', 'ss'], ['e ', 'di', 'ey', 'li', 'nt', 't ', 'ap', 'ic'], ['so', 'a ', ' a', 'ty', ' f', 's ', 'er', ' i'], ['ci', 'an', 'nd', ' t', 'ax', 'op', ' i', 'nc'], ['al', 'd ', ' g', 'o ', ' m', 'po', 'n ', 'lu'], [' r', 'fr', 'ue', 'or', 'ac', 'si', 'th', 'de'], ['el', 'om', 'rn', 'ie', 'hi', 'ti', 'e ', 's '], ['at', ' p', 'se', 'nt', 'ne', 'on', 'un', 'lu'], ['io', 're', 'y ', ' o', 's ', ' a', 'it', 'cy'], ['ns', 'si', 'ha', 'ne', 'wi', ' s', 'ed', ' w'], [' b', 'de', 's ', 'se', 'th', 'ub', ' s', 'in']]\n",
        "  \n",
        "# ons anarch\n",
        "# ists advoc\n",
        "# ate social\n",
        "\n",
        "valid_batches = BatchGenerator(valid_text, 1, 1)\n",
        "print(\"3:\", batches2string( valid_batches.next() ) )\n",
        "print(\"4:\", batches2string( valid_batches.next() ) )\n",
        "# 3: [[' a'], ['na']]\n",
        "# 4: [['na'], ['rc']]\n",
        "\n",
        "# train:  ons anarchists advocate social relations based upon voluntary as\n",
        "# valid:   anarchism originated as a term of abuse first used against earl\n",
        "\n",
        "# オリジナルの実行 bs=8\n",
        "# ['ons anarchi', 'n from the ', 'gnificant t', ' drugs conf', 'of the orig', 'at least no', 'st daily co', 'cky ricardo']\n",
        "# ['ists advoca', ' national m', 'than in jer', 'fusion inab', 'ginal docum', 'ot parliame', 'ollege news', 'o this clas']\n",
        "# [' a']\n",
        "# ['an']"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1: ['ons anarchists advocat', 'when military governme', 'lleria arches national', ' abbeys and monasterie', 'married urraca princes', 'hel and richard baer h', 'y and liturgical langu', 'ay opened for passenge', 'tion from the national', 'migration took place d', 'new york other well kn', 'he boeing seven six se', 'e listed with a gloss ', 'eber has probably been', 'o be made to recognize', 'yer who received the f', 'ore significant than i', 'a fierce critic of the', ' two six eight in sign', 'aristotle s uncaused c', 'ity can be lost as in ', ' and intracellular ice', 'tion of the size of th', 'dy to pass him a stick', 'f certain drugs confus', 'at it will take to com', 'e convince the priest ', 'ent told him to name i', 'ampaign and barred att', 'rver side standard for', 'ious texts such as eso', 'o capitalize on the gr', 'a duplicate of the ori', 'gh ann es d hiver one ', 'ine january eight marc', 'ross zero the lead cha', 'cal theories classical', 'ast instance the non g', ' dimensional analysis ', 'most holy mormons beli', 't s support or at leas', 'u is still disagreed u', 'e oscillating system e', 'o eight subtypes based', 'of italy languages the', 's the tower commission', 'klahoma press one nine', 'erprise linux suse lin', 'ws becomes the first d', 'et in a nazi concentra', 'the fabian society neh', 'etchy to relatively st', ' sharman networks shar', 'ised emperor hirohito ', 'ting in political init', 'd neo latin most of th', 'th risky riskerdoo ric', 'encyclopedic overview ', 'fense the air componen', 'duating from acnm accr', 'treet grid centerline ', 'ations more than any o', 'appeal of devotional b', 'si have made such devi']\n",
            "2: ['ate social relations b', 'ments failed to revive', 'al park photographic v', 'ies index sacred desti', 'ess of castile daughte', ' h provided a detailed', 'guage among jews manda', 'gers in december one n', 'al media and from pres', ' during the one nine e', 'known manufacturers of', 'seven a widebody jet w', 's covering some of the', 'en one of the most inf', 'ze single acts of meri', ' first card from the d', ' in jersey and guernse', 'he poverty and social ', 'gns of humanity vol th', ' cause so aquinas come', 'n denaturalization and', 'ce formation solution ', 'the input usually meas', 'ck to pull him out but', 'usion inability to ori', 'omplete an operation c', 't of the mistakes of a', ' it fort des moines th', 'ttempts by his opponen', 'ormats for mailboxes i', 'soteric christianity a', 'growing popularity of ', 'riginal document fax m', 'e nine eight zero one ', 'rch eight listing of a', 'haracter lieutenant sh', 'al mechanics and speci', ' gm comparison maize c', 's fundamental applicat', 'lieve the configuratio', 'ast not parliament s o', ' upon by historians an', ' example rlc circuit f', 'ed on the whole genome', 'he official language o', 'on at this point presi', 'ne three two one one t', 'inux enterprise server', ' daily college newspap', 'ration camp lewis has ', 'ehru wished the econom', 'stiff from flat to tig', 'arman s sydney based b', 'o to begin negotiation', 'itiatives the lesotho ', 'these authors wrote in', 'icky ricardo this clas', 'w of mathematics prese', 'ent of arm is represen', 'credited programs must', 'e external links bbc o', ' other state modern da', ' buddhism especially r', 'vices possible the sys']\n",
            "3: [' ana']\n",
            "4: ['narc']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "K8f67YXaDr4C",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Simple LSTM Model."
      ]
    },
    {
      "metadata": {
        "id": "Q5rxZK6RDuGe",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 205
        },
        "outputId": "4bf5a4aa-0d53-48be-c044-a7934d45b3de"
      },
      "cell_type": "code",
      "source": [
        "vocabulary_size = embedding_size #1-hotの代わりab_em_vectorを使う\n",
        "\n",
        "num_nodes = 64 #64\n",
        "import tensorflow as tf\n",
        "embedding_size = 128 # Dimension of the embedding vector.\n",
        "\n",
        "graph = tf.Graph()\n",
        "with graph.as_default():\n",
        "\n",
        "  big_x = tf.Variable(tf.truncated_normal([vocabulary_size, num_nodes * 4], -0.1, 0.1), name=\"big_x\")\n",
        "  big_m = tf.Variable(tf.truncated_normal([num_nodes, num_nodes * 4], -0.1, 0.1), name=\"big_m\")\n",
        "  big_b = tf.Variable(tf.zeros([1, num_nodes * 4]), name=\"big_b\")\n",
        "  \n",
        "  # Variables saving state across unrollings.\n",
        "  saved_output = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False,name=\"saved_output\")\n",
        "  saved_state = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False,name=\"saved_state\")\n",
        "  \n",
        "  # Classifier weights and biases.\n",
        "  w = tf.Variable(tf.truncated_normal([num_nodes, vocabulary_size0], -0.1, 0.1),name=\"w\")\n",
        "  b = tf.Variable(tf.zeros([vocabulary_size0]),name=\"b\")\n",
        "  \n",
        "  \n",
        "  # Definition of the cell computation.\n",
        "  def lstm_cell(i, o, state): #receiving input, hidden(t-1) and state(t-1)\n",
        "    \n",
        "    \"\"\"Create a LSTM cell. See e.g.: http://arxiv.org/pdf/1402.1128v1.pdf\n",
        "    Note that in this formulation, we omit the various connections between the\n",
        "    previous state and the gates.\"\"\"\n",
        "    \n",
        "    with tf.name_scope(\"lstm_cell\"):\n",
        "  \n",
        "      split0, split1, split2, split3 = tf.split(tf.matmul(i, big_x) + tf.matmul(o, big_m) + big_b, num_or_size_splits=4, axis=1)\n",
        "      input_gate = tf.sigmoid(split0)\n",
        "      forget_gate = tf.sigmoid(split1)\n",
        "      update = split2\n",
        "      state = forget_gate * state + input_gate * tf.tanh(update) # state(t-1) --> state(t)\n",
        "      output_gate = tf.sigmoid(split3)\n",
        "    return output_gate * tf.tanh(state), state #returning output(t) and state(t)\n",
        " \n",
        "  # Input data.\n",
        "\n",
        "  def tf_bs_em2tf_bs_voc(tf_bs_em): #em vectorからvoc-vecを返す\n",
        "    bs_flat = tf.argmax( tf.matmul( tf_bs_em, final_embeddings_np_voc_em.T ), 1 )\n",
        "    return tf.one_hot( bs_flat, depth=vocabulary_size0 )\n",
        "  \n",
        "  train_data = list() #num_unrollings の展開はListで\n",
        "  \n",
        "  with tf.name_scope(\"train_data_in\"):\n",
        "    for _ in range(num_unrollings + 1):\n",
        "      train_data.append(tf.placeholder(tf.float32, shape=[batch_size,vocabulary_size]))\n",
        " \n",
        "  train_inputs = train_data[:num_unrollings]\n",
        " \n",
        "  train_labels = list()\n",
        "  with tf.name_scope(\"train_data_out\"):\n",
        "    for i in range(1, num_unrollings + 1):\n",
        "      train_labels.append( tf_bs_em2tf_bs_voc( train_data[i] ) )\n",
        "  \n",
        "  # num_unrollings = 3  ->  a, b, c, d \n",
        "  # train_inputs = a, b, c 　  0番目から2番目\n",
        "  # train_labels = b, c, d　  　1番目から3番目(最後)まで\n",
        "  print(\"train_inputs : \", train_inputs)\n",
        "  print(\"train_labels : \", train_labels)\n",
        "  \n",
        "  # Unrolled LSTM loop.\n",
        "  outputs = list()\n",
        "  output = saved_output #????初期値は\n",
        "  state = saved_state\n",
        "  \n",
        "  with tf.name_scope(\"train_inputs\"):\n",
        "    for i in train_inputs: # a, b, c をインプットとして、アウトプットを全部計算する\n",
        "      output, state = lstm_cell(i, output, state)\n",
        "      outputs.append(output)\n",
        "\n",
        "  # State saving across unrollings. logitsとlossの前にsaveをする\n",
        "  with tf.control_dependencies([saved_output.assign(output), saved_state.assign(state)]):\n",
        "    # Classifier.\n",
        "    logits = tf.nn.xw_plus_b(tf.concat(outputs, 0), w, b) # アウトプットから一遍に推論するnp[bs x em]を unroll個縦に 繰り返し #これでクロスエントロピー求めて良い？\n",
        "    loss = tf.reduce_mean( tf.nn.softmax_cross_entropy_with_logits(labels=tf.concat(train_labels, 0), logits=logits), name=\"loss\") #推論と正答の損失関数を計算する\n",
        "#     loss = tf.reduce_mean( tf.square(tf.concat(train_labels, 0)- logits), name=\"loss\") #回帰のようなやり方\n",
        "    print(\"train_labels : \",train_labels)\n",
        "    print(\"labels=tf.concat(train_labels, 0) : \", tf.concat(train_labels, 0))\n",
        "    print(\"logits : \", logits)\n",
        "    print(\"loss : \", loss)\n",
        "\n",
        "  # Optimizer.\n",
        "  global_step = tf.Variable(0, name=\"global_step\")\n",
        "  with tf.name_scope(\"Optimize\"):  #意味\n",
        "    learning_rate = tf.train.exponential_decay(10.0, global_step, 5000, 0.1, staircase=True)\n",
        "    optimizer = tf.train.GradientDescentOptimizer(learning_rate) #learning_rateを入力として、optimizerを初期化する\n",
        "    gradients, v = zip(*optimizer.compute_gradients(loss)) #損失関数を入力として勾配とその変数を返す\n",
        "    print(\"v: \",v) #どの変数について最適化されたか\n",
        "    print(\"global_step: \",global_step) #どうやってインクリメントされるのか？\n",
        "    #pdb.set_trace()\n",
        "    gradients, _ = tf.clip_by_global_norm(gradients, 1.25) #勾配の長さを1.25に調整\n",
        "    optimizer = optimizer.apply_gradients(zip(gradients, v), global_step=global_step)\n",
        "\n",
        "  # Predictions.\n",
        "  train_prediction = tf.nn.softmax(logits,name=\"Predictions\")\n",
        "  print(\"train_prediction : \",train_prediction)\n",
        "  \n",
        "  # Sampling and validation eval: batch 1, no unrolling.\n",
        "  with tf.name_scope(\"Optimize\"):    \n",
        "    sample_input = tf.placeholder(tf.float32, shape=[1, vocabulary_size]) #input\n",
        "    saved_sample_output = tf.Variable(tf.zeros([1, num_nodes]))\n",
        "    saved_sample_state = tf.Variable(tf.zeros([1, num_nodes]))\n",
        "    \n",
        "    # called from 10 timer veri\n",
        "    reset_sample_state = tf.group( \n",
        "        saved_sample_output.assign(tf.zeros([1, num_nodes])), \n",
        "        saved_sample_state.assign(tf.zeros([1, num_nodes])))\n",
        "    \n",
        "    sample_output, sample_state = lstm_cell( sample_input, saved_sample_output, saved_sample_state)\n",
        "    \n",
        "    with tf.control_dependencies([saved_sample_output.assign(sample_output), saved_sample_state.assign(sample_state)]):\n",
        "      sample_prediction = tf.nn.softmax(tf.nn.xw_plus_b(sample_output, w, b)) #called from 10 timer\n",
        "    print(\"sample_prediction : \",sample_prediction)\n",
        "    \n",
        "  #TensorBoard\n",
        "  tf.summary.scalar('Loss', loss) # yをスカラーとして記録\n",
        "  merged = tf.summary.merge_all() # おまじない\n",
        "  logdir = \"./drive/ColabData/logdir\"\n",
        "#   if tf.gfile.Exists(logdir):\n",
        "#     tf.gfile.DeleteRecursively(logdir) # ./logdirが存在する場合削除\n",
        "  writer = tf.summary.FileWriter(logdir, graph) # 保存先を./logdirに設定"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "train_inputs :  [<tf.Tensor 'train_data_in/Placeholder:0' shape=(64, 128) dtype=float32>, <tf.Tensor 'train_data_in/Placeholder_1:0' shape=(64, 128) dtype=float32>, <tf.Tensor 'train_data_in/Placeholder_2:0' shape=(64, 128) dtype=float32>, <tf.Tensor 'train_data_in/Placeholder_3:0' shape=(64, 128) dtype=float32>, <tf.Tensor 'train_data_in/Placeholder_4:0' shape=(64, 128) dtype=float32>, <tf.Tensor 'train_data_in/Placeholder_5:0' shape=(64, 128) dtype=float32>, <tf.Tensor 'train_data_in/Placeholder_6:0' shape=(64, 128) dtype=float32>, <tf.Tensor 'train_data_in/Placeholder_7:0' shape=(64, 128) dtype=float32>, <tf.Tensor 'train_data_in/Placeholder_8:0' shape=(64, 128) dtype=float32>, <tf.Tensor 'train_data_in/Placeholder_9:0' shape=(64, 128) dtype=float32>]\n",
            "train_labels :  [<tf.Tensor 'train_data_out/one_hot:0' shape=(64, 728) dtype=float32>, <tf.Tensor 'train_data_out/one_hot_1:0' shape=(64, 728) dtype=float32>, <tf.Tensor 'train_data_out/one_hot_2:0' shape=(64, 728) dtype=float32>, <tf.Tensor 'train_data_out/one_hot_3:0' shape=(64, 728) dtype=float32>, <tf.Tensor 'train_data_out/one_hot_4:0' shape=(64, 728) dtype=float32>, <tf.Tensor 'train_data_out/one_hot_5:0' shape=(64, 728) dtype=float32>, <tf.Tensor 'train_data_out/one_hot_6:0' shape=(64, 728) dtype=float32>, <tf.Tensor 'train_data_out/one_hot_7:0' shape=(64, 728) dtype=float32>, <tf.Tensor 'train_data_out/one_hot_8:0' shape=(64, 728) dtype=float32>, <tf.Tensor 'train_data_out/one_hot_9:0' shape=(64, 728) dtype=float32>]\n",
            "train_labels :  [<tf.Tensor 'train_data_out/one_hot:0' shape=(64, 728) dtype=float32>, <tf.Tensor 'train_data_out/one_hot_1:0' shape=(64, 728) dtype=float32>, <tf.Tensor 'train_data_out/one_hot_2:0' shape=(64, 728) dtype=float32>, <tf.Tensor 'train_data_out/one_hot_3:0' shape=(64, 728) dtype=float32>, <tf.Tensor 'train_data_out/one_hot_4:0' shape=(64, 728) dtype=float32>, <tf.Tensor 'train_data_out/one_hot_5:0' shape=(64, 728) dtype=float32>, <tf.Tensor 'train_data_out/one_hot_6:0' shape=(64, 728) dtype=float32>, <tf.Tensor 'train_data_out/one_hot_7:0' shape=(64, 728) dtype=float32>, <tf.Tensor 'train_data_out/one_hot_8:0' shape=(64, 728) dtype=float32>, <tf.Tensor 'train_data_out/one_hot_9:0' shape=(64, 728) dtype=float32>]\n",
            "labels=tf.concat(train_labels, 0) :  Tensor(\"concat_2:0\", shape=(640, 728), dtype=float32)\n",
            "logits :  Tensor(\"xw_plus_b:0\", shape=(640, 728), dtype=float32)\n",
            "loss :  Tensor(\"loss:0\", shape=(), dtype=float32)\n",
            "v:  (<tf.Variable 'big_x:0' shape=(128, 256) dtype=float32_ref>, <tf.Variable 'big_m:0' shape=(64, 256) dtype=float32_ref>, <tf.Variable 'big_b:0' shape=(1, 256) dtype=float32_ref>, <tf.Variable 'w:0' shape=(64, 728) dtype=float32_ref>, <tf.Variable 'b:0' shape=(728,) dtype=float32_ref>, <tf.Variable 'global_step:0' shape=() dtype=int32_ref>)\n",
            "global_step:  <tf.Variable 'global_step:0' shape=() dtype=int32_ref>\n",
            "train_prediction :  Tensor(\"Predictions:0\", shape=(640, 728), dtype=float32)\n",
            "sample_prediction :  Tensor(\"Optimize_1/Softmax:0\", shape=(1, 728), dtype=float32)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "KyVd8FxT5QBc",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def np_bs_em2np_bs_voc1hot(np_bs_em): #em vectorからvoc-vecを返す\n",
        "  bs_flat = np.argmax( np.matmul( np_bs_em, final_embeddings_np_voc_em.T ), 1 )\n",
        "  return np.eye( vocabulary_size0 )[bs_flat]\n",
        "\n",
        "def logprob(predictions, labels):\n",
        "#   Log-probability of the true labels in a predicted batch.\n",
        "#   predictions ｂｓ個の出力 emベクトル、softmaxで確率に\n",
        "#   labels ?\n",
        "  predictions[predictions < 1e-10] = 1e-10\n",
        "#   pdb.set_trace()\n",
        "  return np.sum(np.multiply(labels, -np.log(predictions))) / labels.shape[0]\n",
        "\n",
        "def sample_distribution(distribution):\n",
        "#   prediction[0] は　長さ 1のランダムなvoc要素数の１次元配列\n",
        "#   Sample one element from a distribution assumed to be an array of normalized  probabilities.\n",
        "#   結果的に0 から　voc-1 の整数をランダムに生成\n",
        "  r = random.uniform(0, 1)\n",
        "  s = 0\n",
        "  for i in range(len(distribution)):\n",
        "    s += distribution[i]\n",
        "    if s >= r:\n",
        "      return i\n",
        "  return len(distribution) - 1\n",
        "\n",
        "def sample(prediction): #ランダムな文字を　 1-hot で返す　=> abベクトル\n",
        "  # Turn a (column) prediction into 1-hot encoded samples.\n",
        "  # prediction ?\n",
        "  p = np.zeros(shape=[1, vocabulary_size0], dtype=np.float)\n",
        "  p[0, sample_distribution(prediction[0])] = 1.0\n",
        "  return p\n",
        "\n",
        "def random_distribution():\n",
        "  # 長さ 1のランダムなvoc要素数の横ベクトル \n",
        "  b = np.random.uniform(0.0, 1.0, size=[1, vocabulary_size0])\n",
        "#   c =  b/np.sum(b, 1)[:, None]\n",
        "  c =  b/np.sum(b, 1)\n",
        "#   pdb.set_trace()\n",
        "  return c"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "RD9zQCZTEaEm",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 531
        },
        "outputId": "dea8c6cd-e3f4-4be9-e316-83f0f2eaf1d4"
      },
      "cell_type": "code",
      "source": [
        "num_steps = 7001 #7001\n",
        "summary_frequency = 100\n",
        "\n",
        "with tf.Session(graph=graph) as session:\n",
        "  tf.global_variables_initializer().run()\n",
        "  print('Initialized')\n",
        "  mean_loss = 0\n",
        "  \n",
        "  for step in range(num_steps):\n",
        "    \n",
        "    batches = train_batches.next() ### ＜train_batches＞ の利用先\n",
        "    \n",
        "    feed_dict = dict()\n",
        "    for i in range(num_unrollings + 1): \n",
        "      feed_dict[train_data[i]] = batches[i]  #何を入れて学習させたか　追ってみる\n",
        "      \n",
        "    summary, _, l, predictions, lr = session.run( [merged, optimizer, loss, \n",
        "                         train_prediction, learning_rate], feed_dict=feed_dict)\n",
        "    \n",
        "    #predictions = np 80,128   80 = bs x unr 　を np(bs, em)に分解 \n",
        "    pred_batches = list()  #   bs x em のリスト\n",
        "    for unr in range(num_unrollings) : \n",
        "      pred_batches.append(predictions[0 + unr : batch_size + unr,:])\n",
        "\n",
        "    mean_loss +=  l\n",
        "    #print(\"mean_loss\", mean_loss, \"l\", l,\"feed_dict\",feed_dict,\"predictions\",predictions)\n",
        "    \n",
        "    if step % summary_frequency == 0:\n",
        "      if step > 0:\n",
        "        mean_loss = mean_loss / summary_frequency\n",
        "      # The mean loss is an estimate of the loss over the last few batches.\n",
        "      print(\n",
        "        'Average loss at step %d: %f learning rate: %f' % (step, mean_loss, lr))\n",
        "      mean_loss = 0\n",
        "      \n",
        "#       labels = np.concatenate(list(batches)[1:])  ## Lablesが1-hotになった影響を考慮\n",
        "      labels = list()\n",
        "      for unr in range(1, num_unrollings + 1):\n",
        "#         pdb.set_trace()\n",
        "        labels.append( np_bs_em2np_bs_voc1hot(batches[unr]) )\n",
        "\n",
        "      print('Minibatch perplexity: %.2f' % float( np.exp( logprob(predictions, np.concatenate( labels ))))) #ここも1-HOt\n",
        "    \n",
        "      writer.add_summary(summary, step) #TB\n",
        "      \n",
        "      if step % (summary_frequency * 10) == 0:\n",
        "        # Generate some samples.\n",
        "        print('=' * 80)\n",
        "        \n",
        "        for _ in range(1):  #(5)\n",
        "          \n",
        "          char1 = 'j'\n",
        "#           char1 = valid_text[random.randint(0, 999)]  #乱数やめてみる?\n",
        "#           char2 = valid_text[random.randint(0, 999)]\n",
        "          char2 = 't'\n",
        "          \n",
        "          feed =  char2ab_vec_np_em_1( char1, char2).T  #ab形式 2文字のem形式\n",
        "          sentence = char1 + char2  #   先頭にセット\n",
        "          #pdb.set_trace()\n",
        "          reset_sample_state.run()  #ネット状態の初期化\n",
        "          \n",
        "          for _ in range(79):   #前の文字abから次の文字cdを予想、これを文にれんけつし、79回繰り返す\n",
        "            prediction = sample_prediction.eval( {sample_input: feed} )\n",
        "            sentence += np_bs_em2ab_li_bs(prediction)[0] #1-hotから文字にする\n",
        "#             pdb.set_trace()\n",
        "          print(sentence)\n",
        "      \n",
        "    #トレーニングデータを印刷\n",
        "        print(\"inp\",batches2string( batches[0:num_unrollings]), \"ans\",batches2string( batches[1:]), \n",
        "#         print(\"inp\",batches2string( train_data[0:num_unrollings].eval), \"ans\",batches2string( batches[1:]), \n",
        "              \"pre\", batches2string(pred_batches) ) \n",
        "          \n",
        "        print('=' * 80)\n",
        "      # Measure validation set perplexity.\n",
        "      reset_sample_state.run()\n",
        "      valid_logprob = 0\n",
        "      for _ in range(valid_size):\n",
        "        b = valid_batches.next()\n",
        "        predictions = sample_prediction.eval({sample_input: b[0]})\n",
        "        valid_logprob = valid_logprob + logprob(predictions, b[1])\n",
        "      print('Validation set perplexity: %.2f' % float(np.exp(\n",
        "        valid_logprob / valid_size)))"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Initialized\n",
            "Average loss at step 0: 6.589996 learning rate: 10.000000\n",
            "Minibatch perplexity: 727.78\n",
            "================================================================================\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-9-fc6356220a5a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     62\u001b[0m           \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m79\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m   \u001b[0;31m#前の文字abから次の文字cdを予想、これを文にれんけつし、79回繰り返す\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m             \u001b[0mprediction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msample_prediction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0msample_input\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mfeed\u001b[0m\u001b[0;34m}\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 64\u001b[0;31m             \u001b[0msentence\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mnp_bs_em2ab_li_bs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprediction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;31m#1-hotから文字にする\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     65\u001b[0m \u001b[0;31m#             pdb.set_trace()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m           \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentence\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-6-b0fabe9f9846>\u001b[0m in \u001b[0;36mnp_bs_em2ab_li_bs\u001b[0;34m(np_bs_em)\u001b[0m\n\u001b[1;32m     44\u001b[0m   \u001b[0;32mfor\u001b[0m \u001b[0mb\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp_bs_em\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m     \u001b[0mx_np_em\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp_bs_em\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m \u001b[0;34m[\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 46\u001b[0;31m     \u001b[0mab_li_bs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mem_vec2ab\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0mx_np_em\u001b[0m\u001b[0;34m[\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnewaxis\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     47\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mab_li_bs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-5-1a9a286c396f>\u001b[0m in \u001b[0;36mem_vec2ab\u001b[0;34m(np_em_1)\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mem_vec2ab\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp_em_1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;31m#em vectorから文字列を返す\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m   \u001b[0margmax_1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0mfinal_embeddings_np_voc_em\u001b[0m \u001b[0;34m@\u001b[0m \u001b[0mnp_em_1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m \u001b[0;31m#   pdb.set_trace()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mreverse_dictionary\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0margmax_1\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: shapes (728,128) and (728,1) not aligned: 128 (dim 1) != 728 (dim 0)"
          ]
        }
      ]
    },
    {
      "metadata": {
        "id": "Y5tapX3kpcqZ",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "---\n",
        "Problem 3\n",
        "---------\n",
        "\n",
        "(difficult!)\n",
        "\n",
        "Write a sequence-to-sequence LSTM which mirrors all the words in a sentence. For example, if your input is:\n",
        "\n",
        "    the quick brown fox\n",
        "    \n",
        "the model should attempt to output:\n",
        "\n",
        "    eht kciuq nworb xof\n",
        "    \n",
        "Refer to the lecture on how to put together a sequence-to-sequence model, as well as [this article](http://arxiv.org/abs/1409.3215) for best practices.\n",
        "\n",
        "---"
      ]
    }
  ]
}